{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b36e907",
   "metadata": {},
   "source": [
    "# BERT Project with Hugging Face\n",
    "-Joshith Reddy Aleti\n",
    "\n",
    "This notebook walks through fine-tuning, debugging, evaluating, and creatively applying a BERT model using the Hugging Face library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bef03c",
   "metadata": {},
   "source": [
    "## Part 1: Fine-Tuning BERT\n",
    "We will fine-tune `bert-base-uncased` on the IMDb sentiment analysis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974887e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bda6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: tokenize\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True, batch_size=32)\n",
    "tokenized = tokenized.rename_column('label', 'labels')\n",
    "tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized['train'].shuffle(seed=42).select(range(2000)), tokenized['test'].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88576aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144606cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['test'],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabcca6",
   "metadata": {},
   "source": [
    "## Part 2: Debugging Issues\n",
    "If you encounter poor validation performance or overfitting, try adjusting hyperparameters or dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: evaluate before and after adjusting learning rate\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "# If accuracy < 0.8, reduce learning rate or increase batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbfe611",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation Metrics\n",
    "**Tasks:**\n",
    "- Generate predictions on test set\n",
    "- Compute Accuracy, F1-Score, Exact Match (QA), Mean Squared Error (Regression), Log Loss\n",
    "- Reflect on results and refine model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install evaluation libraries\n",
    "!pip install evaluate scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8518656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Load metrics\n",
    "accuracy_metric = evaluate.load('accuracy')\n",
    "f1_metric = evaluate.load('f1')\n",
    "squad_metric = evaluate.load('squad')      # for QA EM and F1\n",
    "mse_metric = evaluate.load('mse')\n",
    "\n",
    "# Assume `predictions` and `labels` from previous classification\n",
    "# e.g., from: predictions = trainer.predict(test_dataset)\n",
    "predictions = trainer.predict(tokenized['test'])\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "preds = np.argmax(logits, axis=1)\n",
    "probs = softmax(logits, axis=1)\n",
    "\n",
    "# Classification metrics\n",
    "acc = accuracy_metric.compute(predictions=preds, references=labels)['accuracy']\n",
    "f1 = f1_metric.compute(predictions=preds, references=labels)['f1']\n",
    "ll = log_loss(labels, probs)\n",
    "\n",
    "print(f\"Classification Accuracy: {acc:.4f}\")\n",
    "print(f\"Classification F1-Score: {f1:.4f}\")\n",
    "print(f\"Classification Log Loss: {ll:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1feb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question-Answering metrics using pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline('question-answering', model='./bert-finetuned-qa', tokenizer='bert-base-uncased')\n",
    "# Example QA data\n",
    "qa_examples = [\n",
    "    {'id': '1', 'context': 'Paris is the capital of France.', 'question': 'What is the capital of France?', 'answers': {'text': ['Paris'], 'answer_start': [0]}},\n",
    "]\n",
    "predictions_qa = []\n",
    "for ex in qa_examples:\n",
    "    out = qa_pipeline({'context': ex['context'], 'question': ex['question']})\n",
    "    predictions_qa.append({'id': ex['id'], 'prediction_text': out['answer']})\n",
    "\n",
    "# Compute QA metrics\n",
    "results_qa = squad_metric.compute(predictions=predictions_qa, references=qa_examples)\n",
    "print(f\"QA Exact Match: {results_qa['exact_match']:.2f}\")\n",
    "print(f\"QA F1: {results_qa['f1']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics on STS-B (Semantic Textual Similarity)\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load STS-B dataset\n",
    "sts = load_dataset('glue', 'stsb')\n",
    "label_key = 'label'\n",
    "\n",
    "# Assume model fine-tuned for regression exists at ./bert-regression-stsb\n",
    "reg_model = BertForSequenceClassification.from_pretrained('./bert-regression-stsb')\n",
    "reg_trainer = Trainer(model=reg_model)\n",
    "# Prepare test texts\n",
    "texts = sts['test']['sentence1']\n",
    "encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "outputs = reg_model(**encodings)\n",
    "preds_reg = outputs.logits.squeeze()\n",
    "refs = np.array(sts['test'][label_key])\n",
    "\n",
    "mse_val = mse_metric.compute(predictions=preds_reg, references=refs)['mse']\n",
    "print(f\"Regression MSE: {mse_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf27ad8",
   "metadata": {},
   "source": [
    "## Part 4: Creative Application â€“ Named Entity Recognition\n",
    "Fine-tune BERT for NER on the CoNLL-2003 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install seqeval for NER evaluation\n",
    "!pip install seqeval evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3be078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load dataset\n",
    "dataset_ner = load_dataset('conll2003')\n",
    "labels = dataset_ner['train'].features['ner_tags'].feature.names\n",
    "tokenizer_ner = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer_ner(examples['tokens'], is_split_into_words=True, truncation=True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_ner = dataset_ner.map(tokenize_and_align_labels, batched=True)\n",
    "model_ner = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(labels))\n",
    "args_ner = TrainingArguments(\n",
    "    output_dir='./results-ner', evaluation_strategy='epoch', per_device_train_batch_size=16, num_train_epochs=3\n",
    ")\n",
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "def compute_metrics_ner(p):\n",
    "    preds, labs = p\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    true_preds = [[labels[p] for (p, l) in zip(pred, lab) if l != -100] for pred, lab in zip(preds, labs)]\n",
    "    true_labels = [[labels[l] for (p, l) in zip(pred, lab) if l != -100] for pred, lab in zip(preds, labs)]\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        'precision': results['overall_precision'],\n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy'],\n",
    "    }\n",
    "\n",
    "trainer_ner = Trainer(\n",
    "    model=model_ner, args=args_ner,\n",
    "    train_dataset=tokenized_ner['train'].select(range(2000)),\n",
    "    eval_dataset=tokenized_ner['validation'].select(range(1000)),\n",
    "    tokenizer=tokenizer_ner,\n",
    "    compute_metrics=compute_metrics_ner\n",
    ")\n",
    "trainer_ner.train()\n",
    "metrics_ner = trainer_ner.evaluate()\n",
    "print(metrics_ner)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
